import torch
import torch.nn as nn
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



from torch import nn
from torch.autograd import Variable
import logging
import pickle
from sklearn.preprocessing import MinMaxScaler

logging.basicConfig(filename = 'price_out_cr.log',level = logging.INFO, format = '%(asctime)s - %(name)s - %(message)s')

#1 数据预处理：
#1.1数据导入转为numpy格式
data_csv = pd.read_csv('price.csv', usecols=[1])  #导入第一列，即价格那一列的数据
data_csv = data_csv.dropna() #去掉na数据
dataset = data_csv.values #dataframe 转为 ndarray
dataset = dataset.astype('float32')
print(dataset.shape) 

logger = logging.getLogger("raw")
logger.info('rawdata:\n{}\n'.format(dataset))
#1.2 数据集分为训练集和测试集
test_data_size = 8
train_data = dataset[:-test_data_size]
test_data = dataset[-test_data_size:]
print(len(train_data)) #52
print(len(test_data)) #8
# 1.3 归一化处理 归一化只能用在训练集而不能用在测试集。如果归一化用在测试集，会有信息从训练集泄露到测试集中。
scaler = MinMaxScaler(feature_range=(-1, 1))
train_data_normalized = scaler.fit_transform(train_data .reshape(-1, 1)) #注意reshape成一列的数据进行归一化
print(train_data_normalized[:5])
print(train_data_normalized[-5:])
# 1.4 下一步是把我们的数据集转换为PyTorch的张量（tensor）：
train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1) #行数不定，列数为tensor的大小 
print(train_data_normalized)
print(train_data_normalized.shape)
logger = logging.getLogger("data")
logger.info('train_data_normalized:\n{}\n'.format(train_data_normalized)) 
# 1.5 制作我们的训练集的对应的标签 
#接下来，我们要定义一个叫create_inout_sequences的函数。这个函数将会接收数据输入的行，然后将会return一个包含元组的列表。在每一个元组中，第一个元素将会包含两个月所对应的价格，第二个元素包含下一个两个月所对应的价格
train_window = 5
def create_inout_sequences(input_data, tw):
    inout_seq = []
    L = len(input_data)
    for i in range(L-tw):
        train_seq = input_data[i:i+tw]
        train_label = input_data[i+tw:i+tw+1]
        inout_seq.append((train_seq ,train_label))
    return inout_seq

train_inout_seq = create_inout_sequences(train_data_normalized, train_window) #创建数据集序列和对应的标签
print(train_inout_seq[:5])
print(len(train_inout_seq))
print('***************************')
logger = logging.getLogger("data")
logger.info('train_inout_seq:\n{}\n'.format(train_inout_seq))
##2.1 搭建lstm模型
class LSTM(nn.Module):
    def __init__(self, input_size=1, hidden_layer_size=6, output_size=1):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size

        self.lstm = nn.LSTM(input_size, hidden_layer_size)

        self.linear = nn.Linear(hidden_layer_size, output_size)

        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),
                            torch.zeros(1,1,self.hidden_layer_size)) # (num_layers * num_directions, batch_size, hidden_size)

    def forward(self, input_seq):
        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)
        predictions = self.linear(lstm_out.view(len(input_seq), -1))
        return predictions[-1]
model = LSTM()
print(model)

loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 100
##2.2 训练模型
for i in range(epochs):
    for seq, labels in train_inout_seq:
        optimizer.zero_grad()  # 将上面运算过程中的grad清零
        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))

        y_pred = model(seq) #####################################

        single_loss = loss_function(y_pred, labels) ##############################
        single_loss.backward() # 误差反向传递
        optimizer.step() # 将新参数作用于神经网络

    if i%25 == 1:

        print('epoch:{:3}   loss:{:.6f}'.format(i,single_loss.data))

print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')

#torch.save(model, 'model.pkl')  # 方式1：保存整个神经网络
torch.save(model.state_dict(), 'model_parameters.pkl')  # 方式2：保存神经网络的参数   
#torch.save(model.state_dict(), PATH)
   

###############################3.模型预测
fut_pred = 8

test_inputs = train_data_normalized[-train_window:].tolist()
print(test_inputs)#初始的测试集包含2项。这2项将会预测测试集的第一项，测试集第一项的数字是133.预测出的值接下来会加入到test_inputs列表的末尾。在第二次循环中，最后的2项又作为输入，一个新的预测值又会加入到test_inputs列表的末尾。这个循环将会执行12次。在循环的最后，test_inputs将会包含24项。最后的12项就是测试集的预测结果。

model.eval()

for i in range(fut_pred):
    seq = torch.FloatTensor(test_inputs[-train_window:]) ##是个张量
    print(seq)
    with torch.no_grad():
        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))
        print(model(seq))
        print(model(seq).item())
        test_inputs.append(model(seq).item())########################## 模型预测时模型的输入
        print(test_inputs)

print(test_inputs) #它包含13个元素。5+8
print(len(test_inputs))#13
print(test_inputs[-fut_pred:]) #最后2个预测结果

actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:] ).reshape(-1, 1))
print(actual_predictions)


#可视化
x = np.arange(52, 60 ,1)
print(x)

plt.title('Month vs Price')
plt.ylabel('Total Price')
plt.grid(True)
plt.autoscale(axis='x', tight=True)
plt.plot(dataset, 'b', label='real')
plt.plot(x,actual_predictions,'r', label='prediction')
plt.legend(loc='best')
plt.show()
plt.savefig('price_result_cr.png')
plt.close()



plt.title('Month vs Price')
plt.ylabel('Total Price')
plt.grid(True)
plt.autoscale(axis='x', tight=True)
plt.plot(x,dataset[-fut_pred:], 'b', label='real')
plt.plot(x,actual_predictions,'r', label='prediction')
plt.legend(loc='best')
plt.savefig('price_result_cr2.png')
plt.close()




